[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Critical Role Analytics",
    "section": "",
    "text": "This is a data analysis site looking into Critical Role, season two, as an example of DnD podcasts. It looks into the player rolls and podcast transcripts to extract insights about how the two effect each other. Much of the data extracted isn’t as strongly coordinated as I might like given the random nature of dice rolls and the unpredictability of actor’s script.\n\n\nData was collected by MATHEUS DE ALBUQUERQUE and posted on Kaggle.com. Additional data on dice rolls was collected by the Critical Rol community and posted on CritRoleStats.com\n\n\n\nFirst things first, I have a table of all of the player rolls and all of the transcript. The rolls needs to reformatted linked to episode transcripts and valueable insights from the text data need to be extracted.\n\n# Include Libraries\nimport glob\nfrom textblob import TextBlob\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndef measure_subjectivity_polarity(text):\n    sentiment = TextBlob(text).sentiment\n    polarity = sentiment.polarity\n    subjectivity = sentiment.subjectivity\n    return subjectivity, polarity\n\n\ndef count_instances(text, word_list):\n    # Stem words (jumping, jumped, jump -&gt; jump)\n    words = word_tokenize(text.lower())\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Count occurrences of common cusswords\n    word_freq = Counter(stemmed_words)\n    instance_count = 0\n    for seek_word in word_list:\n        instance_count += word_freq[seek_word]\n    instance_ratio = instance_count/len(words)\n    return instance_ratio\n\n\nbad_words = ['shit', 'bullshit', 'horseshit', 'fuck', 'motherfuck', 'ass', 'dumbass', 'asshole', 'dick', 'cunt', 'piss','hell', 'damn', 'dammit', 'goddamn', 'goddammit', 'bitch', 'whore', 'prick', 'pussy', 'shite', 'wanker', 'bugger', 'bullocks']\n#'hell' was included while 'bloody' was not because the first was most often used as a swear and the latter was more frequently a literal description\n#Similarly, in D&D it's common for dice to become 'cocked' if they don't land on a flat surface. This caused a lot of false positives from including the swear word 'cock'\n\n\nepisode_list = list(range(1, 109))\nfor index, num in enumerate(episode_list):\n    if num/10 &lt; 1:\n        file_path_number = f\"(2x0{num})\"\n    else:\n        file_path_number = f\"(2x{num})\"\n    data_sheet_number = f\"C2E{num:03}\"\n    episode_list[index] = [file_path_number, data_sheet_number]\n\n\n# Little Helper Function\ndef read_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n            return text\n    except Exception as e:\n        print(f\"Could not read {file_path}: {e}\")\n\n#  Finally, we get to start the fun part with Pandas\ndf = pd.DataFrame()\ndf['episode_number'] = []\ndf['episode_file'] = []\ndf['episode_txt'] = []\nindex = 0\nfor episode in episode_list:\n    matching_files = glob.glob(f\"CR_script_data/{episode[0]}_*\")\n    if not matching_files:\n        print(\"No files matched the pattern.\")\n        print(f\"{episode[0]}_\")\n    else:\n        # Open and process each matching file\n        for file_path in matching_files:\n            episode_text = read_file(file_path)\n            df.loc[index] = pd.Series({'episode_number': episode[1], 'episode_file': file_path.replace('CR_script_data\\\\(','').replace('.txt','').replace(')_',' '), 'episode_txt': episode_text})\n\n            index += 1\n\n\n# This one takes a while to run\ndf['polarity'] = float('NaN')\ndf['subjectivity'] = float('NaN')\nfor index, row in df.iterrows():\n    row_subjectivity, row_polarity = measure_subjectivity_polarity(row['episode_txt'])\n    df.at[index, 'polarity'] = row_polarity\n    df.at[index, 'subjectivity'] = row_subjectivity\n\n\nrolls_df = pd.read_csv(\"CR_rolls_data/All_Rolls_Wildemount_All_Episodes.csv\")\nrolls_df_aggregate = rolls_df[['Episode', 'Total Value', 'Natural Value']].copy()\n\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat20','25')\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat1','0')\nrolls_df_aggregate['Total Value'] = pd.to_numeric(rolls_df_aggregate['Total Value'], errors='coerce')\nrolls_df_aggregate['Natural Value'] = pd.to_numeric(rolls_df_aggregate['Natural Value'], errors='coerce')\n\nrolls_df_aggregate = rolls_df_aggregate.groupby(['Episode']).mean().reset_index()\nrolls_df_aggregate = rolls_df_aggregate.rename(columns={\n    'Episode': 'episode_number',\n    'Total Value': 'mean_rolls',\n    'Natural Value': 'mean_natural_rolls'\n})\nrolls_df_aggregate.head()\n\n\n\n\n\n\n\n\n\nepisode_number\nmean_rolls\nmean_natural_rolls\n\n\n\n\n0\nC2E001\n13.441558\n11.263158\n\n\n1\nC2E002\n15.225000\n11.797468\n\n\n2\nC2E003\n12.877551\n10.358333\n\n\n3\nC2E004\n13.886792\n11.083333\n\n\n4\nC2E005\n13.405660\n10.250000\n\n\n\n\n\n\n\n\n\nnat20_counts = rolls_df[rolls_df['Total Value'] == 'Nat20']\nepisode_nat20_counts = nat20_counts.groupby('Episode')['Total Value'].count().reset_index()\nnat1_counts = rolls_df[rolls_df['Total Value'] == 'Nat1']\nepisode_nat1_counts = nat1_counts.groupby('Episode')['Total Value'].count().reset_index()\n\nepisode_nat20_counts.columns = ['episode_number', 'Nat20_Count']\nepisode_nat1_counts.columns = ['episode_number', 'Nat1_Count']\n\n\ndf['percentage_of_swear_words'] = np.nan\nfor index, row in df.iterrows():\n    swear_count = count_instances(text=row['episode_txt'], word_list=bad_words)\n    df.at[index, 'percentage_of_swear_words'] = swear_count * 100\n\n\ndf = pd.merge(df, rolls_df_aggregate, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat20_counts, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat1_counts, on='episode_number', how='left')\ndf = df.drop(columns=['episode_txt'])\n\nprint(df.head(50))\n\n   episode_number                  episode_file  polarity  subjectivity  \\\n0          C2E001        2x01 CuriousBeginnings  0.101051      0.498707   \n1          C2E002          2x02 AShowofScrutiny  0.066971      0.498220   \n2          C2E003         2x03 TheMidnightChase  0.075693      0.481830   \n3          C2E004          2x04 DisparatePieces  0.086620      0.503113   \n4          C2E005              2x05 TheOpenRoad  0.065001      0.467269   \n5          C2E006          2x06 TheHowlingMines  0.072934      0.469406   \n6          C2E007                     2x07 Hush  0.073154      0.468388   \n7          C2E008         2x08 TheGatesofZadash  0.118077      0.511043   \n8          C2E009     2x09 SteamandConversation  0.099085      0.499854   \n9          C2E010             2x10 WasteandWebs  0.071423      0.494967   \n10         C2E011            2x11 ZemnianNights  0.120496      0.502121   \n11         C2E012        2x12 MidnightEspionage  0.073036      0.490279   \n12         C2E013                2x13 LostFound  0.098318      0.510336   \n13         C2E014         2x14 FleetingMemories  0.079817      0.510299   \n14         C2E015        2x15 WhereTheRiverGoes  0.070542      0.476809   \n15         C2E016             2x16 AFavorinKind  0.060376      0.488762   \n16         C2E017             2x17 HarvestClose  0.074375      0.467971   \n17         C2E018            2x18 WhispersofWar  0.117581      0.496627   \n18         C2E019        2x19 TheGentlemansPath  0.081928      0.485161   \n19         C2E020            2x20 LabendaAwaits  0.067386      0.513579   \n20         C2E021        2x21 StalkerintheSwamp  0.078968      0.508639   \n21         C2E022            2x22 LostTreasures  0.076162      0.487395   \n22         C2E023       2x23 HaveBirdWillTravel  0.083559      0.499407   \n23         C2E024           2x24 TheHourofHonor  0.074451      0.491138   \n24         C2E025           2x25 DivergentPaths  0.093804      0.477025   \n25         C2E026                2x26 FoundLost  0.070814      0.506782   \n26         C2E027           2x27 ConvergingFury  0.082000      0.489123   \n27         C2E028            2x28 WithintheNest  0.090832      0.479271   \n28         C2E029     2x29 TheStalkingNightmare  0.077634      0.479031   \n29         C2E030           2x30 TheJourneyHome  0.080879      0.504778   \n30         C2E031            2x31 CommerceChaos  0.115643      0.487373   \n31         C2E032      2x32 BeyondtheBoundaries  0.103899      0.486623   \n32         C2E033    2x33 TheRubyandtheSapphire  0.135459      0.509550   \n33         C2E034        2x34 EncroachingWaters  0.066119      0.486318   \n34         C2E035        2x35 DocksideDiplomacy  0.102320      0.487896   \n35         C2E036      2x36 OCaptainWhosCaptain  0.099588      0.494401   \n36         C2E037        2x37 DangerousLiaisons  0.080684      0.524244   \n37         C2E038       2x38 WelcometotheJungle  0.091979      0.496377   \n38         C2E039  2x39 TempleoftheFalseSerpent  0.055145      0.486885   \n39         C2E040          2x40 DubiousPursuits  0.101899      0.483645   \n40         C2E041        2x41 APiratesLifeforMe  0.099420      0.520249   \n41         C2E042           2x42 AHoleInthePlan  0.089647      0.518001   \n42         C2E043               2x43 InHotWater  0.083049      0.495164   \n43         C2E044           2x44 TheDiversGrave  0.074075      0.484961   \n44         C2E045              2x45 TheStowaway  0.084919      0.494834   \n45         C2E046         2x46 AStormofMemories  0.090156      0.502699   \n46         C2E047            2x47 TheSecondSeal  0.096302      0.488452   \n47         C2E048            2x48 HomewardBound  0.111892      0.505220   \n48         C2E049             2x49 AGameofNames  0.082316      0.533889   \n49         C2E050        2x50 TheEndlessBurrows  0.076035      0.500989   \n\n    percentage_of_swear_words  mean_rolls  mean_natural_rolls  Nat20_Count  \\\n0                    0.142722   13.441558           11.263158          3.0   \n1                    0.237893   15.225000           11.797468          7.0   \n2                    0.260797   12.877551           10.358333          7.0   \n3                    0.163056   13.886792           11.083333          5.0   \n4                    0.177224   13.405660           10.250000          4.0   \n5                    0.174342   13.928000           11.185567          4.0   \n6                    0.229602   13.265060           10.957265          4.0   \n7                    0.276502   14.166667           11.354167          4.0   \n8                    0.198240   14.320000           10.840000          1.0   \n9                    0.219555   12.640000           10.250000          6.0   \n10                   0.122315   13.980000           11.081633          3.0   \n11                   0.235097   13.346154           10.615385          7.0   \n12                   0.212244   12.213333            9.830769          5.0   \n13                   0.223726   12.393939           10.307692          NaN   \n14                   0.170139   14.214286           11.436893          7.0   \n15                   0.144051   12.443548            9.826923          3.0   \n16                   0.141019   13.628931           10.612903         11.0   \n17                   0.163055   12.895522           10.914894          3.0   \n18                   0.222771   13.500000           10.285714          5.0   \n19                   0.182218   13.947917           10.720588          6.0   \n20                   0.174249   13.494382           10.503759          7.0   \n21                   0.238492   13.065934            9.434109          5.0   \n22                   0.110134   13.742424            9.813084          9.0   \n23                   0.253908   14.436364           11.727273          3.0   \n24                   0.104375   14.679389           10.940000          5.0   \n25                   0.221902   13.572973           10.561644         14.0   \n26                   0.206341   14.942308           10.022222          1.0   \n27                   0.152031   14.775362           11.125000          8.0   \n28                   0.247786   13.618557           10.248175          7.0   \n29                   0.227665   16.529412           12.312500          2.0   \n30                   0.130495   14.244898           11.326087          3.0   \n31                   0.194384   15.437500           11.333333          4.0   \n32                   0.141293   14.416667           10.375000          NaN   \n33                   0.240592   14.254658           10.912000          6.0   \n34                   0.245569   14.479592           10.686567          3.0   \n35                   0.188539   13.989362           11.052632          4.0   \n36                   0.318622   15.280000           11.346154          1.0   \n37                   0.165931   16.110429           10.881119          7.0   \n38                   0.246966   15.788462           11.202703         11.0   \n39                   0.227541   14.640000           11.388235          9.0   \n40                   0.278986   14.830189           11.829787          1.0   \n41                   0.259395   15.369863           10.428571          3.0   \n42                   0.227645   15.827586           11.493671          4.0   \n43                   0.317777   14.096774           10.232394          9.0   \n44                   0.219384   16.189024           11.183206          9.0   \n45                   0.197438   14.278846            9.371795          2.0   \n46                   0.183589   14.431373           11.113636          6.0   \n47                   0.133932   15.566667           11.466667          NaN   \n48                   0.170940   12.625000            9.433333          1.0   \n49                   0.204769   16.958333           11.974790         12.0   \n\n    Nat1_Count  \n0          4.0  \n1          2.0  \n2          7.0  \n3          NaN  \n4          1.0  \n5          1.0  \n6          3.0  \n7          2.0  \n8          1.0  \n9          8.0  \n10         2.0  \n11         5.0  \n12         6.0  \n13         1.0  \n14         5.0  \n15        10.0  \n16        13.0  \n17         3.0  \n18         5.0  \n19         3.0  \n20         7.0  \n21         7.0  \n22         3.0  \n23         1.0  \n24         2.0  \n25         9.0  \n26         4.0  \n27         5.0  \n28        10.0  \n29         NaN  \n30         2.0  \n31         1.0  \n32         NaN  \n33         6.0  \n34         3.0  \n35         2.0  \n36         1.0  \n37         7.0  \n38         1.0  \n39         3.0  \n40         3.0  \n41         3.0  \n42         3.0  \n43         7.0  \n44         7.0  \n45         4.0  \n46         5.0  \n47         1.0  \n48         2.0  \n49         4.0  \n\n\n\n\nFirst, in these bar graphs, ordered by polarity, from overall negative sentiment on the left to overall positive, show the number of Nat 1s and Nat 20s rolled by players, the lowest and highest rolls possible. First, the Nat1 table shows what you might expect, while the overall data is quite varied, you see more Nat 1s on the left, where the sentiment is negative. Interestingly, though, is that you see the same for Nat 20s. The words said by DM and Players continue to become more negative as players roll more Nat 20s.\n\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat1s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\nchart.update_layout(\n    title='Nat20s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 20s rolled'\n)\nchart.show()\n\n                                                \n\n\n                                                \n\n\nThese bar graphs show higher percentage of swear words on the right, cleaner language used on the left. Interestingly, you see once again that the players will swear more on episodes with more Nat 1s but also they seem to swear from excitement on rolling Nat 20s as well.\n\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n\nchart.update_layout(\n    title='Nat1s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat20s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 20s rolled'\n)\n               \nchart.show()\n\n                                                \n\n\n                                                \n\n\nNow to look at how the podcast has changed over the episodes. We can see how the overall subjectivity of the series increased over the season. This reflects how the characters shared more subjective information over the sharing of factual information over the series. Likely this is the effect of the players becoming more comfortable in their roles and populating the podcast more with their subjective lines than the DM’s objective descriptions.\n\ndf_melted = df.sort_values(by=['episode_number'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['subjectivity'],\n                    var_name='metric', value_name='value')\nchart = px.line(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric')\nchart.update_yaxes(range=[0.45, 0.57])  # Replace min_value_y and max_value_y with actual values\n\nchart.update_layout(\n    title='language subjectivity across the season',\n    xaxis_title='Episodes in order of release',\n    yaxis_title='Relative subjectivity of language in episode'\n)\n\nchart.show()"
  },
  {
    "objectID": "index.html#data-used",
    "href": "index.html#data-used",
    "title": "Critical Role Analytics",
    "section": "",
    "text": "Data was collected by MATHEUS DE ALBUQUERQUE and posted on Kaggle.com. Additional data on dice rolls was collected by the Critical Rol community and posted on CritRoleStats.com"
  },
  {
    "objectID": "index.html#data-processing-and-cleaning",
    "href": "index.html#data-processing-and-cleaning",
    "title": "Critical Role Analytics",
    "section": "",
    "text": "First things first, I have a table of all of the player rolls and all of the transcript. The rolls needs to reformatted linked to episode transcripts and valueable insights from the text data need to be extracted.\n\n# Include Libraries\nimport glob\nfrom textblob import TextBlob\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndef measure_subjectivity_polarity(text):\n    sentiment = TextBlob(text).sentiment\n    polarity = sentiment.polarity\n    subjectivity = sentiment.subjectivity\n    return subjectivity, polarity\n\n\ndef count_instances(text, word_list):\n    # Stem words (jumping, jumped, jump -&gt; jump)\n    words = word_tokenize(text.lower())\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Count occurrences of common cusswords\n    word_freq = Counter(stemmed_words)\n    instance_count = 0\n    for seek_word in word_list:\n        instance_count += word_freq[seek_word]\n    instance_ratio = instance_count/len(words)\n    return instance_ratio\n\n\nbad_words = ['shit', 'bullshit', 'horseshit', 'fuck', 'motherfuck', 'ass', 'dumbass', 'asshole', 'dick', 'cunt', 'piss','hell', 'damn', 'dammit', 'goddamn', 'goddammit', 'bitch', 'whore', 'prick', 'pussy', 'shite', 'wanker', 'bugger', 'bullocks']\n#'hell' was included while 'bloody' was not because the first was most often used as a swear and the latter was more frequently a literal description\n#Similarly, in D&D it's common for dice to become 'cocked' if they don't land on a flat surface. This caused a lot of false positives from including the swear word 'cock'\n\n\nepisode_list = list(range(1, 109))\nfor index, num in enumerate(episode_list):\n    if num/10 &lt; 1:\n        file_path_number = f\"(2x0{num})\"\n    else:\n        file_path_number = f\"(2x{num})\"\n    data_sheet_number = f\"C2E{num:03}\"\n    episode_list[index] = [file_path_number, data_sheet_number]\n\n\n# Little Helper Function\ndef read_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n            return text\n    except Exception as e:\n        print(f\"Could not read {file_path}: {e}\")\n\n#  Finally, we get to start the fun part with Pandas\ndf = pd.DataFrame()\ndf['episode_number'] = []\ndf['episode_file'] = []\ndf['episode_txt'] = []\nindex = 0\nfor episode in episode_list:\n    matching_files = glob.glob(f\"CR_script_data/{episode[0]}_*\")\n    if not matching_files:\n        print(\"No files matched the pattern.\")\n        print(f\"{episode[0]}_\")\n    else:\n        # Open and process each matching file\n        for file_path in matching_files:\n            episode_text = read_file(file_path)\n            df.loc[index] = pd.Series({'episode_number': episode[1], 'episode_file': file_path.replace('CR_script_data\\\\(','').replace('.txt','').replace(')_',' '), 'episode_txt': episode_text})\n\n            index += 1\n\n\n# This one takes a while to run\ndf['polarity'] = float('NaN')\ndf['subjectivity'] = float('NaN')\nfor index, row in df.iterrows():\n    row_subjectivity, row_polarity = measure_subjectivity_polarity(row['episode_txt'])\n    df.at[index, 'polarity'] = row_polarity\n    df.at[index, 'subjectivity'] = row_subjectivity\n\n\nrolls_df = pd.read_csv(\"CR_rolls_data/All_Rolls_Wildemount_All_Episodes.csv\")\nrolls_df_aggregate = rolls_df[['Episode', 'Total Value', 'Natural Value']].copy()\n\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat20','25')\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat1','0')\nrolls_df_aggregate['Total Value'] = pd.to_numeric(rolls_df_aggregate['Total Value'], errors='coerce')\nrolls_df_aggregate['Natural Value'] = pd.to_numeric(rolls_df_aggregate['Natural Value'], errors='coerce')\n\nrolls_df_aggregate = rolls_df_aggregate.groupby(['Episode']).mean().reset_index()\nrolls_df_aggregate = rolls_df_aggregate.rename(columns={\n    'Episode': 'episode_number',\n    'Total Value': 'mean_rolls',\n    'Natural Value': 'mean_natural_rolls'\n})\nrolls_df_aggregate.head()\n\n\n\n\n\n\n\n\n\nepisode_number\nmean_rolls\nmean_natural_rolls\n\n\n\n\n0\nC2E001\n13.441558\n11.263158\n\n\n1\nC2E002\n15.225000\n11.797468\n\n\n2\nC2E003\n12.877551\n10.358333\n\n\n3\nC2E004\n13.886792\n11.083333\n\n\n4\nC2E005\n13.405660\n10.250000\n\n\n\n\n\n\n\n\n\nnat20_counts = rolls_df[rolls_df['Total Value'] == 'Nat20']\nepisode_nat20_counts = nat20_counts.groupby('Episode')['Total Value'].count().reset_index()\nnat1_counts = rolls_df[rolls_df['Total Value'] == 'Nat1']\nepisode_nat1_counts = nat1_counts.groupby('Episode')['Total Value'].count().reset_index()\n\nepisode_nat20_counts.columns = ['episode_number', 'Nat20_Count']\nepisode_nat1_counts.columns = ['episode_number', 'Nat1_Count']\n\n\ndf['percentage_of_swear_words'] = np.nan\nfor index, row in df.iterrows():\n    swear_count = count_instances(text=row['episode_txt'], word_list=bad_words)\n    df.at[index, 'percentage_of_swear_words'] = swear_count * 100\n\n\ndf = pd.merge(df, rolls_df_aggregate, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat20_counts, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat1_counts, on='episode_number', how='left')\ndf = df.drop(columns=['episode_txt'])\n\nprint(df.head(50))\n\n   episode_number                  episode_file  polarity  subjectivity  \\\n0          C2E001        2x01 CuriousBeginnings  0.101051      0.498707   \n1          C2E002          2x02 AShowofScrutiny  0.066971      0.498220   \n2          C2E003         2x03 TheMidnightChase  0.075693      0.481830   \n3          C2E004          2x04 DisparatePieces  0.086620      0.503113   \n4          C2E005              2x05 TheOpenRoad  0.065001      0.467269   \n5          C2E006          2x06 TheHowlingMines  0.072934      0.469406   \n6          C2E007                     2x07 Hush  0.073154      0.468388   \n7          C2E008         2x08 TheGatesofZadash  0.118077      0.511043   \n8          C2E009     2x09 SteamandConversation  0.099085      0.499854   \n9          C2E010             2x10 WasteandWebs  0.071423      0.494967   \n10         C2E011            2x11 ZemnianNights  0.120496      0.502121   \n11         C2E012        2x12 MidnightEspionage  0.073036      0.490279   \n12         C2E013                2x13 LostFound  0.098318      0.510336   \n13         C2E014         2x14 FleetingMemories  0.079817      0.510299   \n14         C2E015        2x15 WhereTheRiverGoes  0.070542      0.476809   \n15         C2E016             2x16 AFavorinKind  0.060376      0.488762   \n16         C2E017             2x17 HarvestClose  0.074375      0.467971   \n17         C2E018            2x18 WhispersofWar  0.117581      0.496627   \n18         C2E019        2x19 TheGentlemansPath  0.081928      0.485161   \n19         C2E020            2x20 LabendaAwaits  0.067386      0.513579   \n20         C2E021        2x21 StalkerintheSwamp  0.078968      0.508639   \n21         C2E022            2x22 LostTreasures  0.076162      0.487395   \n22         C2E023       2x23 HaveBirdWillTravel  0.083559      0.499407   \n23         C2E024           2x24 TheHourofHonor  0.074451      0.491138   \n24         C2E025           2x25 DivergentPaths  0.093804      0.477025   \n25         C2E026                2x26 FoundLost  0.070814      0.506782   \n26         C2E027           2x27 ConvergingFury  0.082000      0.489123   \n27         C2E028            2x28 WithintheNest  0.090832      0.479271   \n28         C2E029     2x29 TheStalkingNightmare  0.077634      0.479031   \n29         C2E030           2x30 TheJourneyHome  0.080879      0.504778   \n30         C2E031            2x31 CommerceChaos  0.115643      0.487373   \n31         C2E032      2x32 BeyondtheBoundaries  0.103899      0.486623   \n32         C2E033    2x33 TheRubyandtheSapphire  0.135459      0.509550   \n33         C2E034        2x34 EncroachingWaters  0.066119      0.486318   \n34         C2E035        2x35 DocksideDiplomacy  0.102320      0.487896   \n35         C2E036      2x36 OCaptainWhosCaptain  0.099588      0.494401   \n36         C2E037        2x37 DangerousLiaisons  0.080684      0.524244   \n37         C2E038       2x38 WelcometotheJungle  0.091979      0.496377   \n38         C2E039  2x39 TempleoftheFalseSerpent  0.055145      0.486885   \n39         C2E040          2x40 DubiousPursuits  0.101899      0.483645   \n40         C2E041        2x41 APiratesLifeforMe  0.099420      0.520249   \n41         C2E042           2x42 AHoleInthePlan  0.089647      0.518001   \n42         C2E043               2x43 InHotWater  0.083049      0.495164   \n43         C2E044           2x44 TheDiversGrave  0.074075      0.484961   \n44         C2E045              2x45 TheStowaway  0.084919      0.494834   \n45         C2E046         2x46 AStormofMemories  0.090156      0.502699   \n46         C2E047            2x47 TheSecondSeal  0.096302      0.488452   \n47         C2E048            2x48 HomewardBound  0.111892      0.505220   \n48         C2E049             2x49 AGameofNames  0.082316      0.533889   \n49         C2E050        2x50 TheEndlessBurrows  0.076035      0.500989   \n\n    percentage_of_swear_words  mean_rolls  mean_natural_rolls  Nat20_Count  \\\n0                    0.142722   13.441558           11.263158          3.0   \n1                    0.237893   15.225000           11.797468          7.0   \n2                    0.260797   12.877551           10.358333          7.0   \n3                    0.163056   13.886792           11.083333          5.0   \n4                    0.177224   13.405660           10.250000          4.0   \n5                    0.174342   13.928000           11.185567          4.0   \n6                    0.229602   13.265060           10.957265          4.0   \n7                    0.276502   14.166667           11.354167          4.0   \n8                    0.198240   14.320000           10.840000          1.0   \n9                    0.219555   12.640000           10.250000          6.0   \n10                   0.122315   13.980000           11.081633          3.0   \n11                   0.235097   13.346154           10.615385          7.0   \n12                   0.212244   12.213333            9.830769          5.0   \n13                   0.223726   12.393939           10.307692          NaN   \n14                   0.170139   14.214286           11.436893          7.0   \n15                   0.144051   12.443548            9.826923          3.0   \n16                   0.141019   13.628931           10.612903         11.0   \n17                   0.163055   12.895522           10.914894          3.0   \n18                   0.222771   13.500000           10.285714          5.0   \n19                   0.182218   13.947917           10.720588          6.0   \n20                   0.174249   13.494382           10.503759          7.0   \n21                   0.238492   13.065934            9.434109          5.0   \n22                   0.110134   13.742424            9.813084          9.0   \n23                   0.253908   14.436364           11.727273          3.0   \n24                   0.104375   14.679389           10.940000          5.0   \n25                   0.221902   13.572973           10.561644         14.0   \n26                   0.206341   14.942308           10.022222          1.0   \n27                   0.152031   14.775362           11.125000          8.0   \n28                   0.247786   13.618557           10.248175          7.0   \n29                   0.227665   16.529412           12.312500          2.0   \n30                   0.130495   14.244898           11.326087          3.0   \n31                   0.194384   15.437500           11.333333          4.0   \n32                   0.141293   14.416667           10.375000          NaN   \n33                   0.240592   14.254658           10.912000          6.0   \n34                   0.245569   14.479592           10.686567          3.0   \n35                   0.188539   13.989362           11.052632          4.0   \n36                   0.318622   15.280000           11.346154          1.0   \n37                   0.165931   16.110429           10.881119          7.0   \n38                   0.246966   15.788462           11.202703         11.0   \n39                   0.227541   14.640000           11.388235          9.0   \n40                   0.278986   14.830189           11.829787          1.0   \n41                   0.259395   15.369863           10.428571          3.0   \n42                   0.227645   15.827586           11.493671          4.0   \n43                   0.317777   14.096774           10.232394          9.0   \n44                   0.219384   16.189024           11.183206          9.0   \n45                   0.197438   14.278846            9.371795          2.0   \n46                   0.183589   14.431373           11.113636          6.0   \n47                   0.133932   15.566667           11.466667          NaN   \n48                   0.170940   12.625000            9.433333          1.0   \n49                   0.204769   16.958333           11.974790         12.0   \n\n    Nat1_Count  \n0          4.0  \n1          2.0  \n2          7.0  \n3          NaN  \n4          1.0  \n5          1.0  \n6          3.0  \n7          2.0  \n8          1.0  \n9          8.0  \n10         2.0  \n11         5.0  \n12         6.0  \n13         1.0  \n14         5.0  \n15        10.0  \n16        13.0  \n17         3.0  \n18         5.0  \n19         3.0  \n20         7.0  \n21         7.0  \n22         3.0  \n23         1.0  \n24         2.0  \n25         9.0  \n26         4.0  \n27         5.0  \n28        10.0  \n29         NaN  \n30         2.0  \n31         1.0  \n32         NaN  \n33         6.0  \n34         3.0  \n35         2.0  \n36         1.0  \n37         7.0  \n38         1.0  \n39         3.0  \n40         3.0  \n41         3.0  \n42         3.0  \n43         7.0  \n44         7.0  \n45         4.0  \n46         5.0  \n47         1.0  \n48         2.0  \n49         4.0  \n\n\n\n\nFirst, in these bar graphs, ordered by polarity, from overall negative sentiment on the left to overall positive, show the number of Nat 1s and Nat 20s rolled by players, the lowest and highest rolls possible. First, the Nat1 table shows what you might expect, while the overall data is quite varied, you see more Nat 1s on the left, where the sentiment is negative. Interestingly, though, is that you see the same for Nat 20s. The words said by DM and Players continue to become more negative as players roll more Nat 20s.\n\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat1s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\nchart.update_layout(\n    title='Nat20s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 20s rolled'\n)\nchart.show()\n\n                                                \n\n\n                                                \n\n\nThese bar graphs show higher percentage of swear words on the right, cleaner language used on the left. Interestingly, you see once again that the players will swear more on episodes with more Nat 1s but also they seem to swear from excitement on rolling Nat 20s as well.\n\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n\nchart.update_layout(\n    title='Nat1s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat20s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 20s rolled'\n)\n               \nchart.show()\n\n                                                \n\n\n                                                \n\n\nNow to look at how the podcast has changed over the episodes. We can see how the overall subjectivity of the series increased over the season. This reflects how the characters shared more subjective information over the sharing of factual information over the series. Likely this is the effect of the players becoming more comfortable in their roles and populating the podcast more with their subjective lines than the DM’s objective descriptions.\n\ndf_melted = df.sort_values(by=['episode_number'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['subjectivity'],\n                    var_name='metric', value_name='value')\nchart = px.line(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric')\nchart.update_yaxes(range=[0.45, 0.57])  # Replace min_value_y and max_value_y with actual values\n\nchart.update_layout(\n    title='language subjectivity across the season',\n    xaxis_title='Episodes in order of release',\n    yaxis_title='Relative subjectivity of language in episode'\n)\n\nchart.show()"
  }
]
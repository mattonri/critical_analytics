[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Critical Role Analytics",
    "section": "",
    "text": "This is a data analysis site looking into Critical Role, season two, as an example of DnD podcasts. It looks into the player rolls and podcast transcripts to extract insights about how the two effect each other. Much of the data extracted isn’t as strongly coordinated as I might like given the random nature of dice rolls and the unpredictability of actor’s script.\n\n\nData was collected by MATHEUS DE ALBUQUERQUE and posted on Kaggle.com. Additional data on dice rolls was collected by the Critical Rol community and posted on CritRoleStats.com\n\n\n\nFirst things first, I have a table of all of the player rolls and all of the transcript. The rolls needs to reformatted linked to episode transcripts and valueable insights from the text data need to be extracted.\n\n# Include Libraries\nimport glob\nfrom textblob import TextBlob\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndef measure_subjectivity_polarity(text):\n    sentiment = TextBlob(text).sentiment\n    polarity = sentiment.polarity\n    subjectivity = sentiment.subjectivity\n    return subjectivity, polarity\n\n\ndef count_instances(text, word_list):\n    # Stem words (jumping, jumped, jump -&gt; jump)\n    words = word_tokenize(text.lower())\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Count occurrences of common cusswords\n    word_freq = Counter(stemmed_words)\n    instance_count = 0\n    for seek_word in word_list:\n        instance_count += word_freq[seek_word]\n    instance_ratio = instance_count/len(words)\n    return instance_ratio\n\n\nbad_words = ['shit', 'bullshit', 'horseshit', 'fuck', 'motherfuck', 'ass', 'dumbass', 'asshole', 'dick', 'cunt', 'piss','hell', 'damn', 'dammit', 'goddamn', 'goddammit', 'bitch', 'whore', 'prick', 'pussy', 'shite', 'wanker', 'bugger', 'bullocks']\n#'hell' was included while 'bloody' was not because the first was most often used as a swear and the latter was more frequently a literal description\n#Similarly, in D&D it's common for dice to become 'cocked' if they don't land on a flat surface. This caused a lot of false positives from including the swear word 'cock'\n\n\nepisode_list = list(range(1, 109))\nfor index, num in enumerate(episode_list):\n    if num/10 &lt; 1:\n        file_path_number = f\"(2x0{num})\"\n    else:\n        file_path_number = f\"(2x{num})\"\n    data_sheet_number = f\"C2E{num:03}\"\n    episode_list[index] = [file_path_number, data_sheet_number]\n\n\n# Little Helper Function\ndef read_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n            return text\n    except Exception as e:\n        print(f\"Could not read {file_path}: {e}\")\n\n#  Finally, we get to start the fun part with Pandas\ndf = pd.DataFrame()\ndf['episode_number'] = []\ndf['episode_file'] = []\ndf['episode_txt'] = []\nindex = 0\nfor episode in episode_list:\n    matching_files = glob.glob(f\"CR_script_data/{episode[0]}_*\")\n    if not matching_files:\n        print(\"No files matched the pattern.\")\n        print(f\"{episode[0]}_\")\n    else:\n        # Open and process each matching file\n        for file_path in matching_files:\n            episode_text = read_file(file_path)\n            df.loc[index] = pd.Series({'episode_number': episode[1], 'episode_file': file_path.replace('CR_script_data\\\\(','').replace('.txt','').replace(')_',' '), 'episode_txt': episode_text})\n\n            index += 1\n\n\n# This one takes a while to run\ndf['polarity'] = float('NaN')\ndf['subjectivity'] = float('NaN')\nfor index, row in df.iterrows():\n    row_subjectivity, row_polarity = measure_subjectivity_polarity(row['episode_txt'])\n    df.at[index, 'polarity'] = row_polarity\n    df.at[index, 'subjectivity'] = row_subjectivity\n\n\nrolls_df = pd.read_csv(\"CR_rolls_data/All_Rolls_Wildemount_All_Episodes.csv\")\nrolls_df_aggregate = rolls_df[['Episode', 'Total Value', 'Natural Value']].copy()\n\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat20','25')\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat1','0')\nrolls_df_aggregate['Total Value'] = pd.to_numeric(rolls_df_aggregate['Total Value'], errors='coerce')\nrolls_df_aggregate['Natural Value'] = pd.to_numeric(rolls_df_aggregate['Natural Value'], errors='coerce')\n\nrolls_df_aggregate = rolls_df_aggregate.groupby(['Episode']).mean().reset_index()\nrolls_df_aggregate = rolls_df_aggregate.rename(columns={\n    'Episode': 'episode_number',\n    'Total Value': 'mean_rolls',\n    'Natural Value': 'mean_natural_rolls'\n})\nrolls_df_aggregate.head()\n\n\n\n\n\n\n\n\n\nepisode_number\nmean_rolls\nmean_natural_rolls\n\n\n\n\n0\nC2E001\n13.441558\n11.263158\n\n\n1\nC2E002\n15.225000\n11.797468\n\n\n2\nC2E003\n12.877551\n10.358333\n\n\n3\nC2E004\n13.886792\n11.083333\n\n\n4\nC2E005\n13.405660\n10.250000\n\n\n\n\n\n\n\n\n\nnat20_counts = rolls_df[rolls_df['Total Value'] == 'Nat20']\nepisode_nat20_counts = nat20_counts.groupby('Episode')['Total Value'].count().reset_index()\nnat1_counts = rolls_df[rolls_df['Total Value'] == 'Nat1']\nepisode_nat1_counts = nat1_counts.groupby('Episode')['Total Value'].count().reset_index()\n\nepisode_nat20_counts.columns = ['episode_number', 'Nat20_Count']\nepisode_nat1_counts.columns = ['episode_number', 'Nat1_Count']\n\n\ndf['percentage_of_swear_words'] = np.nan\nfor index, row in df.iterrows():\n    swear_count = count_instances(text=row['episode_txt'], word_list=bad_words)\n    df.at[index, 'percentage_of_swear_words'] = swear_count * 100\n\nOnce everything is put together, we’ll have a table of data that looks like the following:\n\ndf = pd.merge(df, rolls_df_aggregate, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat20_counts, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat1_counts, on='episode_number', how='left')\ndf = df.drop(columns=['episode_txt'])\n\nprint(df.head(1))\n\n  episode_number            episode_file  polarity  subjectivity  \\\n0         C2E001  2x01 CuriousBeginnings  0.101051      0.498707   \n\n   percentage_of_swear_words  mean_rolls  mean_natural_rolls  Nat20_Count  \\\n0                   0.142722   13.441558           11.263158          3.0   \n\n   Nat1_Count  \n0         4.0  \n\n\n\n\nFirst, in these bar graphs, ordered by polarity, from overall negative sentiment on the left to overall positive, show the number of Nat 1s and Nat 20s rolled by players, the lowest and highest rolls possible. First, the Nat1 table shows what you might expect, while the overall data is quite varied, you see more Nat 1s on the left, where the sentiment is negative. Interestingly, though, is that you see the same for Nat 20s. The words said by DM and Players continue to become more negative as players roll more Nat 20s.\n\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat1s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\nchart.update_layout(\n    title='Nat20s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 20s rolled'\n)\nchart.show()\n\n                                                \n\n\n                                                \n\n\nThese bar graphs show higher percentage of swear words on the right, cleaner language used on the left. Interestingly, you see once again that the players will swear more on episodes with more Nat 1s but also they seem to swear from excitement on rolling Nat 20s as well.\n\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n\nchart.update_layout(\n    title='Nat1s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat20s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 20s rolled'\n)\n               \nchart.show()\n\n                                                \n\n\n                                                \n\n\nNow to look at how the podcast has changed over the episodes. We can see how the overall subjectivity of the series increased over the season. This reflects how the characters shared more subjective information over the sharing of factual information over the series. Likely this is the effect of the players becoming more comfortable in their roles and populating the podcast more with their subjective lines than the DM’s objective descriptions.\n\ndf_melted = df.sort_values(by=['episode_number'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['subjectivity'],\n                    var_name='metric', value_name='value')\nchart = px.line(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric')\nchart.update_yaxes(range=[0.45, 0.57])  # Replace min_value_y and max_value_y with actual values\n\nchart.update_layout(\n    title='language subjectivity across the season',\n    xaxis_title='Episodes in order of release',\n    yaxis_title='Relative subjectivity of language in episode'\n)\n\nchart.show()"
  },
  {
    "objectID": "index.html#data-used",
    "href": "index.html#data-used",
    "title": "Critical Role Analytics",
    "section": "",
    "text": "Data was collected by MATHEUS DE ALBUQUERQUE and posted on Kaggle.com. Additional data on dice rolls was collected by the Critical Rol community and posted on CritRoleStats.com"
  },
  {
    "objectID": "index.html#data-processing-and-cleaning",
    "href": "index.html#data-processing-and-cleaning",
    "title": "Critical Role Analytics",
    "section": "",
    "text": "First things first, I have a table of all of the player rolls and all of the transcript. The rolls needs to reformatted linked to episode transcripts and valueable insights from the text data need to be extracted.\n\n# Include Libraries\nimport glob\nfrom textblob import TextBlob\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\n\ndef measure_subjectivity_polarity(text):\n    sentiment = TextBlob(text).sentiment\n    polarity = sentiment.polarity\n    subjectivity = sentiment.subjectivity\n    return subjectivity, polarity\n\n\ndef count_instances(text, word_list):\n    # Stem words (jumping, jumped, jump -&gt; jump)\n    words = word_tokenize(text.lower())\n    stemmer = PorterStemmer()\n    stemmed_words = [stemmer.stem(word) for word in words]\n\n    # Count occurrences of common cusswords\n    word_freq = Counter(stemmed_words)\n    instance_count = 0\n    for seek_word in word_list:\n        instance_count += word_freq[seek_word]\n    instance_ratio = instance_count/len(words)\n    return instance_ratio\n\n\nbad_words = ['shit', 'bullshit', 'horseshit', 'fuck', 'motherfuck', 'ass', 'dumbass', 'asshole', 'dick', 'cunt', 'piss','hell', 'damn', 'dammit', 'goddamn', 'goddammit', 'bitch', 'whore', 'prick', 'pussy', 'shite', 'wanker', 'bugger', 'bullocks']\n#'hell' was included while 'bloody' was not because the first was most often used as a swear and the latter was more frequently a literal description\n#Similarly, in D&D it's common for dice to become 'cocked' if they don't land on a flat surface. This caused a lot of false positives from including the swear word 'cock'\n\n\nepisode_list = list(range(1, 109))\nfor index, num in enumerate(episode_list):\n    if num/10 &lt; 1:\n        file_path_number = f\"(2x0{num})\"\n    else:\n        file_path_number = f\"(2x{num})\"\n    data_sheet_number = f\"C2E{num:03}\"\n    episode_list[index] = [file_path_number, data_sheet_number]\n\n\n# Little Helper Function\ndef read_file(file_path):\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            text = file.read()\n            return text\n    except Exception as e:\n        print(f\"Could not read {file_path}: {e}\")\n\n#  Finally, we get to start the fun part with Pandas\ndf = pd.DataFrame()\ndf['episode_number'] = []\ndf['episode_file'] = []\ndf['episode_txt'] = []\nindex = 0\nfor episode in episode_list:\n    matching_files = glob.glob(f\"CR_script_data/{episode[0]}_*\")\n    if not matching_files:\n        print(\"No files matched the pattern.\")\n        print(f\"{episode[0]}_\")\n    else:\n        # Open and process each matching file\n        for file_path in matching_files:\n            episode_text = read_file(file_path)\n            df.loc[index] = pd.Series({'episode_number': episode[1], 'episode_file': file_path.replace('CR_script_data\\\\(','').replace('.txt','').replace(')_',' '), 'episode_txt': episode_text})\n\n            index += 1\n\n\n# This one takes a while to run\ndf['polarity'] = float('NaN')\ndf['subjectivity'] = float('NaN')\nfor index, row in df.iterrows():\n    row_subjectivity, row_polarity = measure_subjectivity_polarity(row['episode_txt'])\n    df.at[index, 'polarity'] = row_polarity\n    df.at[index, 'subjectivity'] = row_subjectivity\n\n\nrolls_df = pd.read_csv(\"CR_rolls_data/All_Rolls_Wildemount_All_Episodes.csv\")\nrolls_df_aggregate = rolls_df[['Episode', 'Total Value', 'Natural Value']].copy()\n\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat20','25')\nrolls_df_aggregate['Total Value'] = rolls_df_aggregate['Total Value'].str.replace('Nat1','0')\nrolls_df_aggregate['Total Value'] = pd.to_numeric(rolls_df_aggregate['Total Value'], errors='coerce')\nrolls_df_aggregate['Natural Value'] = pd.to_numeric(rolls_df_aggregate['Natural Value'], errors='coerce')\n\nrolls_df_aggregate = rolls_df_aggregate.groupby(['Episode']).mean().reset_index()\nrolls_df_aggregate = rolls_df_aggregate.rename(columns={\n    'Episode': 'episode_number',\n    'Total Value': 'mean_rolls',\n    'Natural Value': 'mean_natural_rolls'\n})\nrolls_df_aggregate.head()\n\n\n\n\n\n\n\n\n\nepisode_number\nmean_rolls\nmean_natural_rolls\n\n\n\n\n0\nC2E001\n13.441558\n11.263158\n\n\n1\nC2E002\n15.225000\n11.797468\n\n\n2\nC2E003\n12.877551\n10.358333\n\n\n3\nC2E004\n13.886792\n11.083333\n\n\n4\nC2E005\n13.405660\n10.250000\n\n\n\n\n\n\n\n\n\nnat20_counts = rolls_df[rolls_df['Total Value'] == 'Nat20']\nepisode_nat20_counts = nat20_counts.groupby('Episode')['Total Value'].count().reset_index()\nnat1_counts = rolls_df[rolls_df['Total Value'] == 'Nat1']\nepisode_nat1_counts = nat1_counts.groupby('Episode')['Total Value'].count().reset_index()\n\nepisode_nat20_counts.columns = ['episode_number', 'Nat20_Count']\nepisode_nat1_counts.columns = ['episode_number', 'Nat1_Count']\n\n\ndf['percentage_of_swear_words'] = np.nan\nfor index, row in df.iterrows():\n    swear_count = count_instances(text=row['episode_txt'], word_list=bad_words)\n    df.at[index, 'percentage_of_swear_words'] = swear_count * 100\n\nOnce everything is put together, we’ll have a table of data that looks like the following:\n\ndf = pd.merge(df, rolls_df_aggregate, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat20_counts, on='episode_number', how='left')\ndf = pd.merge(df, episode_nat1_counts, on='episode_number', how='left')\ndf = df.drop(columns=['episode_txt'])\n\nprint(df.head(1))\n\n  episode_number            episode_file  polarity  subjectivity  \\\n0         C2E001  2x01 CuriousBeginnings  0.101051      0.498707   \n\n   percentage_of_swear_words  mean_rolls  mean_natural_rolls  Nat20_Count  \\\n0                   0.142722   13.441558           11.263158          3.0   \n\n   Nat1_Count  \n0         4.0  \n\n\n\n\nFirst, in these bar graphs, ordered by polarity, from overall negative sentiment on the left to overall positive, show the number of Nat 1s and Nat 20s rolled by players, the lowest and highest rolls possible. First, the Nat1 table shows what you might expect, while the overall data is quite varied, you see more Nat 1s on the left, where the sentiment is negative. Interestingly, though, is that you see the same for Nat 20s. The words said by DM and Players continue to become more negative as players roll more Nat 20s.\n\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat1s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['polarity'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\nchart.update_layout(\n    title='Nat20s across increasingly negative sentiment',\n    xaxis_title='Episodes in order of increasing negativity',\n    yaxis_title='Nat 20s rolled'\n)\nchart.show()\n\n                                                \n\n\n                                                \n\n\nThese bar graphs show higher percentage of swear words on the right, cleaner language used on the left. Interestingly, you see once again that the players will swear more on episodes with more Nat 1s but also they seem to swear from excitement on rolling Nat 20s as well.\n\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat1_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n\nchart.update_layout(\n    title='Nat1s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 1s rolled'\n)\n\nchart.show()\ndf_melted = df.sort_values(by=['percentage_of_swear_words'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['Nat20_Count'],\n                    var_name='metric', value_name='value')\nchart = px.bar(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric',\n               barmode='group')\n               \nchart.update_layout(\n    title='Nat20s across increasingly swearing episodes',\n    xaxis_title='Episodes in order of increasing swearing',\n    yaxis_title='Nat 20s rolled'\n)\n               \nchart.show()\n\n                                                \n\n\n                                                \n\n\nNow to look at how the podcast has changed over the episodes. We can see how the overall subjectivity of the series increased over the season. This reflects how the characters shared more subjective information over the sharing of factual information over the series. Likely this is the effect of the players becoming more comfortable in their roles and populating the podcast more with their subjective lines than the DM’s objective descriptions.\n\ndf_melted = df.sort_values(by=['episode_number'])\ndf_melted = df_melted.melt(id_vars=['episode_number'], value_vars=['subjectivity'],\n                    var_name='metric', value_name='value')\nchart = px.line(df_melted,\n               x='episode_number',\n               y='value',\n               color='metric')\nchart.update_yaxes(range=[0.45, 0.57])  # Replace min_value_y and max_value_y with actual values\n\nchart.update_layout(\n    title='language subjectivity across the season',\n    xaxis_title='Episodes in order of release',\n    yaxis_title='Relative subjectivity of language in episode'\n)\n\nchart.show()"
  }
]